{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised, Supervised, Finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Unsupervised data (just text)\n",
    "unsupervised_data = pd.DataFrame({\"text\": [\"Your text data for pretraining\"]})\n",
    "unsupervised_dataset = Dataset.from_pandas(unsupervised_data)\n",
    "\n",
    "# Supervised data (text with labels)\n",
    "supervised_data = pd.DataFrame({\n",
    "    \"text\": [\"Input text for fine-tuning\"],\n",
    "    \"label\": [1]  # Your labels (e.g., 1, 0 for binary classification)\n",
    "})\n",
    "supervised_dataset = Dataset.from_pandas(supervised_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa880cc6315d4459997881e297b153cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7745e135c54b0689462ef530645848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 5.8666, 'train_samples_per_second': 0.17, 'train_steps_per_second': 0.17, 'train_loss': 6.993592262268066, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./unsupervised_model/tokenizer_config.json',\n",
       " './unsupervised_model/special_tokens_map.json',\n",
       " './unsupervised_model/vocab.json',\n",
       " './unsupervised_model/merges.txt',\n",
       " './unsupervised_model/added_tokens.json',\n",
       " './unsupervised_model/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "# Load the pretrained tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Use eos_token as the pad_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Tokenize the unsupervised dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], return_special_tokens_mask=True, truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "unsupervised_dataset = unsupervised_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Data collator for language modeling (this automatically creates the labels from input_ids)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Set to False because GPT-2 is a causal language model, not masked\n",
    ")\n",
    "\n",
    "# Pretraining setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=unsupervised_dataset,\n",
    "    data_collator=data_collator,  # Ensure labels are created\n",
    ")\n",
    "\n",
    "# Train the model (pretraining)\n",
    "trainer.train()\n",
    "\n",
    "# Save the pretrained model\n",
    "model.save_pretrained(\"./unsupervised_model\")\n",
    "tokenizer.save_pretrained(\"./unsupervised_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in dataset: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of samples in dataset: {len(supervised_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at ./unsupervised_model and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa563fc54164db4a89a2fe0356f8b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd438e7246147039c8fb3cbdd8d50e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2.7097, 'train_samples_per_second': 1.107, 'train_steps_per_second': 1.107, 'train_loss': 9.622931798299154, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./finetuned_model/tokenizer_config.json',\n",
       " './finetuned_model/special_tokens_map.json',\n",
       " './finetuned_model/vocab.json',\n",
       " './finetuned_model/merges.txt',\n",
       " './finetuned_model/added_tokens.json',\n",
       " './finetuned_model/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Supervised data (text with labels) - one sentence\n",
    "supervised_data = pd.DataFrame({\n",
    "    \"text\": [\"Input text for fine-tuning\"],  # Your single input text\n",
    "    \"label\": [1]  # Binary label\n",
    "})\n",
    "supervised_dataset = Dataset.from_pandas(supervised_data)\n",
    "\n",
    "# Load the pretrained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./unsupervised_model\")\n",
    "\n",
    "# Ensure that the tokenizer uses the eos_token as the pad_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./unsupervised_model\", num_labels=2)\n",
    "\n",
    "# Tokenize the supervised dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "# Apply tokenization\n",
    "supervised_dataset = supervised_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Data collator for padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Fine-tuning setup (skip evaluation since we only have one sample)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_finetune\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    evaluation_strategy=\"no\",  # Skip evaluation\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=supervised_dataset,  # Use the entire dataset for training\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./finetuned_model\")\n",
    "tokenizer.save_pretrained(\"./finetuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.9999710321426392}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load fine-tuned model\n",
    "classifier = pipeline(\"text-classification\", model=\"./finetuned_model\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = classifier(\"Your input text for prediction\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'negative', 'score': 0.9999710321426392}]\n"
     ]
    }
   ],
   "source": [
    "label_map = {0: \"negative\", 1: \"positive\"}  # Adjust this mapping to match your labels\n",
    "\n",
    "# Process the prediction output\n",
    "for prediction in predictions:\n",
    "    label_id = int(prediction['label'].split('_')[-1])  # Extract the numeric part of the label\n",
    "    prediction['label'] = label_map[label_id]\n",
    "\n",
    "print(predictions)  # Updated output with meaningful labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.9999710321426392}]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"text-classification\", model=\"./finetuned_model\", device=0)  # Use GPU (if available)\n",
    "predictions = classifier(\"Your input text for prediction\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.999996542930603}]\n"
     ]
    }
   ],
   "source": [
    "new_input = \"This is another input text to classify again\"\n",
    "predictions = classifier(new_input)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "99% sure we are label_0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
